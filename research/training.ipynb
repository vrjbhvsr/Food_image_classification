{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\49179\\\\Desktop\\\\Food_image_classification\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\49179\\\\Desktop\\\\Food_image_classification'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class training_config:\n",
    "    root_dir: Path\n",
    "    model_path: Path\n",
    "    epochs: int\n",
    "    batch_size: int\n",
    "    learning_rate: float\n",
    "    device: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Food_Classification.utils.common import read_yaml, create_directory\n",
    "from Food_Classification.constants import *\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, \n",
    "                 config_file_path= CONFIG_FILE_PATH,\n",
    "                 params_file_path = PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        create_directory([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_training_config(self) -> training_config:\n",
    "        config = self.config.model_training\n",
    "        create_directory([config.root_dir])\n",
    "        self.model_path = self.config.prepare_base_model.updated_base_model\n",
    "        Training_Config = training_config(root_dir= config.root_dir,\n",
    "                                          model_path= self.model_path,\n",
    "                                          epochs= self.params.EPOCHS,\n",
    "                                          batch_size= self.params.BATCH_SIZE,\n",
    "                                          learning_rate= self.params.LEARNING_RATE,\n",
    "                                          device=self.params.DEVICE\n",
    "                                         )\n",
    "\n",
    "\n",
    "        return Training_Config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\49179\\anaconda3\\envs\\Food\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#%%writefile src/Food_Classification/components/model_trainier.py\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from Food_Classification import logger\n",
    "from Food_Classification.config.configuration import training_config\n",
    "from Food_Classification.entity.artifact_entity import DataTransformationArtifact\n",
    "\n",
    "\n",
    "\n",
    "class Model_training:\n",
    "    def __init__(self, config: training_config, \n",
    "                        artifact: DataTransformationArtifact,\n",
    "                        loss_function: torch.nn.Module, \n",
    "                        optimizer: torch.optim.Optimizer, \n",
    "                        writer: SummaryWriter):\n",
    "        self.config = config\n",
    "        self.artifact: DataTransformationArtifact = artifact\n",
    "        self.train_dataloader = self.artifact[0]\n",
    "        self.test_dataloader = self.artifact[1]\n",
    "        self.model = torch.load(self.config.model_path,map_location=torch.device(self.config.device))\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.writer = writer\n",
    "    \n",
    "    def train_step(self):\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer(params= self.model.parameters(),lr = self.config.learning_rate)\n",
    "        try:\n",
    "            # Initiate model training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            train_acc= 0\n",
    "            progress = tqdm(self.train_dataloader)\n",
    "            # Looping through batches\n",
    "            for batch, (data, target) in enumerate(progress):\n",
    "                # Setting up data into device\n",
    "                data, target = data.to(self.config.device), target.to(self.config.device)\n",
    "\n",
    "                # forward propogation\n",
    "                target_pred = model(data)\n",
    "\n",
    "                # Calculating loss\n",
    "                loss = self.loss_function(target_pred, target)\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # setting optimizer to zero gradient\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # backward propogation\n",
    "                loss.backward()\n",
    "                \n",
    "                # updating weights\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "                \n",
    "                # Calculating accuracy\n",
    "                target_pred_class = torch.argmax(torch.softmax(target_pred, dim=1), dim=1) \n",
    "                train_acc += (target_pred_class == target).sum().item() / len(target_pred)\n",
    "\n",
    "            train_loss /= len(self.train_dataloader)\n",
    "            train_acc /= len(self.train_dataloader)\n",
    "            progress.set_description(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "            return train_loss, train_acc\n",
    "                \n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def test_step(self):\n",
    "        model = self.model\n",
    "        try:\n",
    "            # Intiatig model evaluation\n",
    "            model.eval()\n",
    "\n",
    "            test_loss, test_acc = 0,0\n",
    "            progress = tqdm(self.test_dataloader)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                for batch, (data, target) in enumerate(progress):\n",
    "                    data, target = data.to(self.config.device), target.to(self.config.device)\n",
    "\n",
    "                    # forward propogation\n",
    "                    target_pred = model(data)\n",
    "\n",
    "                    # Calculating Loss\n",
    "                    loss = self.loss_function(target_pred, target)\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    # calculating Accuracy\n",
    "                    target_pred_labels = target_pred.argmax(dim=1)\n",
    "                    test_acc += ((target_pred_labels) == target).sum().item()/len(target_pred_labels)\n",
    "\n",
    "            test_loss = test_loss/len(self.test_dataloader)\n",
    "            test_acc = test_acc /len(self.test_dataloader)\n",
    "            return test_loss, test_acc\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def initiate_training(self):\n",
    "        model = self.config.model_path\n",
    "        results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "        }\n",
    "\n",
    "        for epoch in tqdm(range(self.config.epochs)):\n",
    "            train_loss, train_acc = self.train_step()\n",
    "            test_loss, test_acc = self.test_step()\n",
    "\n",
    "            logger.info(f\"Epoch: {epoch+1} | \"\n",
    "                f\"self.train_loss: {train_loss:.4f} | \"\n",
    "                f\"self.train_acc: {train_acc:.4f} | \"\n",
    "                f\"self.test_loss: {test_loss:.4f} | \"\n",
    "                f\"self.test_acc: {test_acc:.4f}\")\n",
    "\n",
    "            # Update results dictionary\n",
    "            results[\"train_loss\"].append(train_loss)\n",
    "            results[\"train_acc\"].append(train_acc)\n",
    "            results[\"test_loss\"].append(test_loss)\n",
    "            results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        # Save the model\n",
    "        trained_model = os.path.join(self.config.root_dir, \"trained_model.pth\")\n",
    "        torch.save(model,trained_model)\n",
    "\n",
    "         ### Experiment tracking\n",
    "        self.writer.add_scalars(main_tag=\"Loss\",\n",
    "                            tag_scalar_dict = {\"Train_loss\" : train_loss,\n",
    "                                                \"Test_loss\" : test_loss},\n",
    "                            global_step= epoch\n",
    "                            )\n",
    "        self.writer.add_scalars(main_tag=\"Accuracy\",\n",
    "                            tag_scalar_dict = {\"Train_acc\" : train_acc,\n",
    "                                                \"Test_acc\" : test_acc},\n",
    "                            global_step= epoch\n",
    "                            )\n",
    "\n",
    "       # self.writer.add_graph(model=model,\n",
    "       # input_to_model = torch.randn(32,3,224,224).to(self.config.device))\n",
    "    # Return the filled results at the end of the epochs\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-07 21:13:34,544: INFO: common: yaml file config\\config.yaml loaded successfully]\n",
      "[2024-05-07 21:13:34,547: INFO: common: yaml file params.yaml loaded successfully]\n",
      "[2024-05-07 21:13:34,547: INFO: common: directory artifacts created successfully]\n",
      "[2024-05-07 21:13:34,548: INFO: common: directory artifacts/data_transforms created successfully]\n",
      "[2024-05-07 21:13:34,549: INFO: data_transformation: Initiating data transformation]\n",
      "[2024-05-07 21:13:34,549: INFO: data_transformation: Transforming train data]\n",
      "[2024-05-07 21:13:34,550: INFO: data_transformation: Transforming test data]\n",
      "[2024-05-07 21:13:34,554: INFO: data_transformation: creating_dataloaders]\n",
      "[2024-05-07 21:13:34,589: INFO: data_transformation: DataLoaders created]\n",
      "[2024-05-07 21:13:34,590: INFO: common: directory artifacts/prepare_tensorboard created successfully]\n",
      "[2024-05-07 21:13:34,591: INFO: common: directory artifacts/prepare_tensorboard/tensorboard_log_dir created successfully]\n",
      "[2024-05-07 21:13:34,595: INFO: common: directory artifacts/prepare_base_model created successfully]\n",
      "[2024-05-07 21:13:35,565: INFO: prepare_base_model: ============================================================================================================================================\n",
      "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
      "============================================================================================================================================\n",
      "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 20]              --                   Partial\n",
      "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1792, 7, 7]      --                   False\n",
      "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 48, 112, 112]    --                   False\n",
      "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 48, 112, 112]    (1,296)              False\n",
      "│    │    └─BatchNorm2d (1)                                  [1, 48, 112, 112]    [1, 48, 112, 112]    (96)                 False\n",
      "│    │    └─SiLU (2)                                         [1, 48, 112, 112]    [1, 48, 112, 112]    --                   --\n",
      "│    └─Sequential (1)                                        [1, 48, 112, 112]    [1, 24, 112, 112]    --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 48, 112, 112]    [1, 24, 112, 112]    (2,940)              False\n",
      "│    │    └─MBConv (1)                                       [1, 24, 112, 112]    [1, 24, 112, 112]    (1,206)              False\n",
      "│    └─Sequential (2)                                        [1, 24, 112, 112]    [1, 32, 56, 56]      --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 24, 112, 112]    [1, 32, 56, 56]      (11,878)             False\n",
      "│    │    └─MBConv (1)                                       [1, 32, 56, 56]      [1, 32, 56, 56]      (18,120)             False\n",
      "│    │    └─MBConv (2)                                       [1, 32, 56, 56]      [1, 32, 56, 56]      (18,120)             False\n",
      "│    │    └─MBConv (3)                                       [1, 32, 56, 56]      [1, 32, 56, 56]      (18,120)             False\n",
      "│    └─Sequential (3)                                        [1, 32, 56, 56]      [1, 56, 28, 28]      --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 32, 56, 56]      [1, 56, 28, 28]      (25,848)             False\n",
      "│    │    └─MBConv (1)                                       [1, 56, 28, 28]      [1, 56, 28, 28]      (57,246)             False\n",
      "│    │    └─MBConv (2)                                       [1, 56, 28, 28]      [1, 56, 28, 28]      (57,246)             False\n",
      "│    │    └─MBConv (3)                                       [1, 56, 28, 28]      [1, 56, 28, 28]      (57,246)             False\n",
      "│    └─Sequential (4)                                        [1, 56, 28, 28]      [1, 112, 14, 14]     --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 56, 28, 28]      [1, 112, 14, 14]     (70,798)             False\n",
      "│    │    └─MBConv (1)                                       [1, 112, 14, 14]     [1, 112, 14, 14]     (197,820)            False\n",
      "│    │    └─MBConv (2)                                       [1, 112, 14, 14]     [1, 112, 14, 14]     (197,820)            False\n",
      "│    │    └─MBConv (3)                                       [1, 112, 14, 14]     [1, 112, 14, 14]     (197,820)            False\n",
      "│    │    └─MBConv (4)                                       [1, 112, 14, 14]     [1, 112, 14, 14]     (197,820)            False\n",
      "│    │    └─MBConv (5)                                       [1, 112, 14, 14]     [1, 112, 14, 14]     (197,820)            False\n",
      "│    └─Sequential (5)                                        [1, 112, 14, 14]     [1, 160, 14, 14]     --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 112, 14, 14]     [1, 160, 14, 14]     (240,924)            False\n",
      "│    │    └─MBConv (1)                                       [1, 160, 14, 14]     [1, 160, 14, 14]     (413,160)            False\n",
      "│    │    └─MBConv (2)                                       [1, 160, 14, 14]     [1, 160, 14, 14]     (413,160)            False\n",
      "│    │    └─MBConv (3)                                       [1, 160, 14, 14]     [1, 160, 14, 14]     (413,160)            False\n",
      "│    │    └─MBConv (4)                                       [1, 160, 14, 14]     [1, 160, 14, 14]     (413,160)            False\n",
      "│    │    └─MBConv (5)                                       [1, 160, 14, 14]     [1, 160, 14, 14]     (413,160)            False\n",
      "│    └─Sequential (6)                                        [1, 160, 14, 14]     [1, 272, 7, 7]       --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 160, 14, 14]     [1, 272, 7, 7]       (520,904)            False\n",
      "│    │    └─MBConv (1)                                       [1, 272, 7, 7]       [1, 272, 7, 7]       (1,159,332)          False\n",
      "│    │    └─MBConv (2)                                       [1, 272, 7, 7]       [1, 272, 7, 7]       (1,159,332)          False\n",
      "│    │    └─MBConv (3)                                       [1, 272, 7, 7]       [1, 272, 7, 7]       (1,159,332)          False\n",
      "│    │    └─MBConv (4)                                       [1, 272, 7, 7]       [1, 272, 7, 7]       (1,159,332)          False\n",
      "│    │    └─MBConv (5)                                       [1, 272, 7, 7]       [1, 272, 7, 7]       (1,159,332)          False\n",
      "│    │    └─MBConv (6)                                       [1, 272, 7, 7]       [1, 272, 7, 7]       (1,159,332)          False\n",
      "│    │    └─MBConv (7)                                       [1, 272, 7, 7]       [1, 272, 7, 7]       (1,159,332)          False\n",
      "│    └─Sequential (7)                                        [1, 272, 7, 7]       [1, 448, 7, 7]       --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 272, 7, 7]       [1, 448, 7, 7]       (1,420,804)          False\n",
      "│    │    └─MBConv (1)                                       [1, 448, 7, 7]       [1, 448, 7, 7]       (3,049,200)          False\n",
      "│    └─Conv2dNormActivation (8)                              [1, 448, 7, 7]       [1, 1792, 7, 7]      --                   False\n",
      "│    │    └─Conv2d (0)                                       [1, 448, 7, 7]       [1, 1792, 7, 7]      (802,816)            False\n",
      "│    │    └─BatchNorm2d (1)                                  [1, 1792, 7, 7]      [1, 1792, 7, 7]      (3,584)              False\n",
      "│    │    └─SiLU (2)                                         [1, 1792, 7, 7]      [1, 1792, 7, 7]      --                   --\n",
      "├─AdaptiveAvgPool2d (avgpool)                                [1, 1792, 7, 7]      [1, 1792, 1, 1]      --                   --\n",
      "├─Sequential (classifier)                                    [1, 1792]            [1, 20]              --                   True\n",
      "│    └─Dropout (0)                                           [1, 1792]            [1, 1792]            --                   --\n",
      "│    └─Linear (1)                                            [1, 1792]            [1, 20]              35,860               True\n",
      "============================================================================================================================================\n",
      "Total params: 17,584,476\n",
      "Trainable params: 35,860\n",
      "Non-trainable params: 17,548,616\n",
      "Total mult-adds (G): 1.50\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 272.49\n",
      "Params size (MB): 70.34\n",
      "Estimated Total Size (MB): 343.43\n",
      "============================================================================================================================================]\n",
      "[2024-05-07 21:13:35,743: INFO: common: directory artifacts/model_training created successfully]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [01:35<00:00,  1.96it/s]\n",
      "100%|██████████| 63/63 [01:13<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-07 21:16:27,271: INFO: 3855471529: Epoch: 1 | self.train_loss: 2.6828 | self.train_acc: 0.2924 | self.test_loss: 2.3945 | self.test_acc: 0.4529]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:51<00:00, 171.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [2.6827739616657826],\n",
       " 'train_acc': [0.2923869680851064],\n",
       " 'test_loss': [2.394515571140108],\n",
       " 'test_acc': [0.45287698412698413]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Food_Classification.components.data_transformation import data_transformation\n",
    "from Food_Classification.components.tensorboard import preparetensorboard\n",
    "from Food_Classification.components.prepare_base_model import PrepareBaseModel\n",
    "from Food_Classification.config.configuration import ConfigurationManager\n",
    "config = ConfigurationManager()\n",
    "transformation_config = config.get_data_transform_config()\n",
    "transform = data_transformation(config=transformation_config)\n",
    "data_transformation_artifact = transform.initiate_data_transformation()\n",
    "tensorboard = preparetensorboard(config=config.get_tensorboard_config())\n",
    "writer = tensorboard.get_summary_writer()\n",
    "prepare_base_model_config = config.get_base_model_config()\n",
    "prepare_base_model = PrepareBaseModel(config=prepare_base_model_config)\n",
    "prepare_base_model.get_base_model()\n",
    "prepare_base_model.update_base_model()\n",
    "train_config = config.get_training_config()\n",
    "training = Model_training(train_config, artifact=data_transformation_artifact, loss_function= torch.nn.CrossEntropyLoss(), optimizer= torch.optim.Adam,writer=writer)\n",
    "training.initiate_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\Food_Classification\\pipeline\\stage_4_Model_training_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\Food_Classification\\pipeline\\stage_4_Model_training_pipeline.py\n",
    "from Food_Classification.config.configuration import ConfigurationManager\n",
    "from Food_Classification.components.tensorboard import preparetensorboard\n",
    "from Food_Classification.components.model_trainier import Model_training\n",
    "from Food_Classification import logger\n",
    "\n",
    "STAGE_NAME = \"Model Training\"\n",
    "\n",
    "class ModelTrainingPipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        try:\n",
    "            config = ConfigurationManager()\n",
    "            tensorboard = preparetensorboard(config=config.get_tensorboard_config())\n",
    "            writer = tensorboard.get_summary_writer()\n",
    "            train_config = config.get_training_config()\n",
    "            training = Model_training(train_config, artifact=data_transformation_artifact, loss_function= torch.nn.CrossEntropyLoss(), optimizer= torch.optim.Adam,writer=writer)\n",
    "            training.initiate_training()\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(f\">>>>>>> stage : {STAGE_NAME} <<<<<<<<\")\n",
    "        Preapare_base_model = ModelTrainingPipeline()\n",
    "        Preapare_base_model.main()\n",
    "        logger.info(f\">>>>>>> stage : {STAGE_NAME} completed <<<<<<<< \\n\\nx========x\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
